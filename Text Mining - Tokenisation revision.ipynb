{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improting nltk\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample Text:\n",
    "\n",
    "sample_text = '''Trump was born and raised in the New York City borough of Queens,\n",
    "and received an economics degree from the Wharton School of the University of\n",
    "Pennsylvania. He took charge of his family's real estate business in 1971,\n",
    "renamed it to The Trump Organization, and expanded it into Manhattan.\n",
    "The company built or renovated skyscrapers, hotels, casinos, and golf courses.\n",
    "Trump later started various side ventures, including licensing his name\n",
    "for real estate and consumer products. He managed the company until his\n",
    "2017 inauguration. He co-authored several books, including The Art of the Deal.\n",
    "He owned the Miss Universe and Miss USA beauty pageants from 1996 to 2015,\n",
    "and he produced and hosted the reality television show The Apprentice from\n",
    "2003 to 2015. Forbes estimates his net worth to be $3.1 billion.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump was born and raised in the New York City borough of Queens,\n",
      "and received an economics degree from the Wharton School of the University of\n",
      "Pennsylvania.\n",
      "\n",
      "He took charge of his family's real estate business in 1971,\n",
      "renamed it to The Trump Organization, and expanded it into Manhattan.\n",
      "\n",
      "The company built or renovated skyscrapers, hotels, casinos, and golf courses.\n",
      "\n",
      "Trump later started various side ventures, including licensing his name\n",
      "for real estate and consumer products.\n",
      "\n",
      "He managed the company until his\n",
      "2017 inauguration.\n",
      "\n",
      "He co-authored several books, including The Art of the Deal.\n",
      "\n",
      "He owned the Miss Universe and Miss USA beauty pageants from 1996 to 2015,\n",
      "and he produced and hosted the reality television show The Apprentice from\n",
      "2003 to 2015.\n",
      "\n",
      "Forbes estimates his net worth to be $3.1 billion.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Tokenizing into a list of sentences\n",
    "\n",
    "for i in sent_tokenize(sample_text):\n",
    "    print (i+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trump', 'was', 'born', 'and', 'raised', 'in', 'the', 'New', 'York', 'City', 'borough', 'of', 'Queens', ',', 'and', 'received', 'an', 'economics', 'degree', 'from', 'the', 'Wharton', 'School', 'of', 'the', 'University', 'of', 'Pennsylvania', '.', 'He', 'took', 'charge', 'of', 'his', 'family', \"'s\", 'real', 'estate', 'business', 'in', '1971', ',', 'renamed', 'it', 'to', 'The', 'Trump', 'Organization', ',', 'and', 'expanded', 'it', 'into', 'Manhattan', '.', 'The', 'company', 'built', 'or', 'renovated', 'skyscrapers', ',', 'hotels', ',', 'casinos', ',', 'and', 'golf', 'courses', '.', 'Trump', 'later', 'started', 'various', 'side', 'ventures', ',', 'including', 'licensing', 'his', 'name', 'for', 'real', 'estate', 'and', 'consumer', 'products', '.', 'He', 'managed', 'the', 'company', 'until', 'his', '2017', 'inauguration', '.', 'He', 'co-authored', 'several', 'books', ',', 'including', 'The', 'Art', 'of', 'the', 'Deal', '.', 'He', 'owned', 'the', 'Miss', 'Universe', 'and', 'Miss', 'USA', 'beauty', 'pageants', 'from', '1996', 'to', '2015', ',', 'and', 'he', 'produced', 'and', 'hosted', 'the', 'reality', 'television', 'show', 'The', 'Apprentice', 'from', '2003', 'to', '2015', '.', 'Forbes', 'estimates', 'his', 'net', 'worth', 'to', 'be', '$', '3.1', 'billion', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing into a list of words:\n",
    "\n",
    "print(word_tokenize(sample_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words in the sentence with index 0 is 29 \n",
      " The total number of unique words is  24 \n",
      "\n",
      "The total number of words in the sentence with index 1 is 26 \n",
      " The total number of unique words is  24 \n",
      "\n",
      "The total number of words in the sentence with index 2 is 15 \n",
      " The total number of unique words is  13 \n",
      "\n",
      "The total number of words in the sentence with index 3 is 18 \n",
      " The total number of unique words is  18 \n",
      "\n",
      "The total number of words in the sentence with index 4 is 9 \n",
      " The total number of unique words is  9 \n",
      "\n",
      "The total number of words in the sentence with index 5 is 12 \n",
      " The total number of unique words is  12 \n",
      "\n",
      "The total number of words in the sentence with index 6 is 31 \n",
      " The total number of unique words is  24 \n",
      "\n",
      "The total number of words in the sentence with index 7 is 11 \n",
      " The total number of unique words is  11 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the set of each tokenized sentence\n",
    "# Set eliminates duplicates and helps us identify unique ones\n",
    "\n",
    "for i in sent_tokenize(sample_text):\n",
    "    words = word_tokenize(i)\n",
    "    length = len(words)\n",
    "    unique = len(set(words))\n",
    "    print ('The total number of words in the sentence with index', sent_tokenize(sample_text).index(i),'is',length, \n",
    "           '\\n The total number of unique words is ', unique, '\\n' )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a list of unique words\n",
    "words = word_tokenize(sample_text)\n",
    "unique_words = list(dict.fromkeys(words))\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 --> Trump\n",
      "1 --> was\n",
      "1 --> born\n",
      "8 --> and\n",
      "1 --> raised\n",
      "2 --> in\n",
      "7 --> the\n",
      "1 --> New\n",
      "1 --> York\n",
      "1 --> City\n",
      "1 --> borough\n",
      "5 --> of\n",
      "1 --> Queens\n",
      "9 --> ,\n",
      "1 --> received\n",
      "1 --> an\n",
      "1 --> economics\n",
      "1 --> degree\n",
      "3 --> from\n",
      "1 --> Wharton\n",
      "1 --> School\n",
      "1 --> University\n",
      "1 --> Pennsylvania\n",
      "8 --> .\n",
      "4 --> He\n",
      "1 --> took\n",
      "1 --> charge\n",
      "4 --> his\n",
      "1 --> family\n",
      "1 --> 's\n",
      "2 --> real\n",
      "2 --> estate\n",
      "1 --> business\n",
      "1 --> 1971\n",
      "1 --> renamed\n",
      "2 --> it\n",
      "4 --> to\n",
      "4 --> The\n",
      "1 --> Organization\n",
      "1 --> expanded\n",
      "1 --> into\n",
      "1 --> Manhattan\n",
      "2 --> company\n",
      "1 --> built\n",
      "1 --> or\n",
      "1 --> renovated\n",
      "1 --> skyscrapers\n",
      "1 --> hotels\n",
      "1 --> casinos\n",
      "1 --> golf\n",
      "1 --> courses\n",
      "1 --> later\n",
      "1 --> started\n",
      "1 --> various\n",
      "1 --> side\n",
      "1 --> ventures\n",
      "2 --> including\n",
      "1 --> licensing\n",
      "1 --> name\n",
      "1 --> for\n",
      "1 --> consumer\n",
      "1 --> products\n",
      "1 --> managed\n",
      "1 --> until\n",
      "1 --> 2017\n",
      "1 --> inauguration\n",
      "1 --> co-authored\n",
      "1 --> several\n",
      "1 --> books\n",
      "1 --> Art\n",
      "1 --> Deal\n",
      "1 --> owned\n",
      "2 --> Miss\n",
      "1 --> Universe\n",
      "1 --> USA\n",
      "1 --> beauty\n",
      "1 --> pageants\n",
      "1 --> 1996\n",
      "2 --> 2015\n",
      "1 --> he\n",
      "1 --> produced\n",
      "1 --> hosted\n",
      "1 --> reality\n",
      "1 --> television\n",
      "1 --> show\n",
      "1 --> Apprentice\n",
      "1 --> 2003\n",
      "1 --> Forbes\n",
      "1 --> estimates\n",
      "1 --> net\n",
      "1 --> worth\n",
      "1 --> be\n",
      "1 --> $\n",
      "1 --> 3.1\n",
      "1 --> billion\n",
      "[(',', 9), ('and', 8), ('.', 8), ('the', 7), ('of', 5)]\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(words)\n",
    "for i in fdist:\n",
    "    print (fdist[i], '-->', i)\n",
    "print(fdist.most_common(5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trump', 'born', 'raised', 'New', 'York', 'City', 'borough', 'Queens', 'received', 'economics', 'degree', 'Wharton', 'School', 'University', 'Pennsylvania', 'He', 'took', 'charge', 'family', \"'s\", 'real', 'estate', 'business', '1971', 'renamed', 'The', 'Trump', 'Organization', 'expanded', 'Manhattan', 'The', 'company', 'built', 'renovated', 'skyscrapers', 'hotels', 'casinos', 'golf', 'courses', 'Trump', 'later', 'started', 'various', 'side', 'ventures', 'including', 'licensing', 'name', 'real', 'estate', 'consumer', 'products', 'He', 'managed', 'company', '2017', 'inauguration', 'He', 'co-authored', 'several', 'books', 'including', 'The', 'Art', 'Deal', 'He', 'owned', 'Miss', 'Universe', 'Miss', 'USA', 'beauty', 'pageants', '1996', '2015', 'produced', 'hosted', 'reality', 'television', 'show', 'The', 'Apprentice', '2003', '2015', 'Forbes', 'estimates', 'net', 'worth', '3.1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "filtered_sample = []\n",
    "for i in word_tokenize(sample_text):\n",
    "    if i not in stop_words:\n",
    "        if i not in punctuation:\n",
    "            filtered_sample.append(i)\n",
    "print (filtered_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 4), ('The', 4), ('Trump', 3), ('real', 2), ('estate', 2), ('company', 2)]\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(filtered_sample)\n",
    "print(fdist.most_common(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game\n",
      "game\n",
      "game\n",
      "game\n"
     ]
    }
   ],
   "source": [
    "# Stemming \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "word1=[\"game\",\"gaming\",\"gamed\",\"games\"]\n",
    "word2=[\"learn\",\"learning\",\"learns\",\"learned\",\"learner\"]\n",
    "word3=[\"do\",\"doing\",\"done\",\"did\",\"does\"]\n",
    "\n",
    "for word in word1:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It:It\n",
      "is:is\n",
      "important:import\n",
      "to:to\n",
      "learn:learn\n",
      "what:what\n",
      "you:you\n",
      "are:are\n",
      "learning:learn\n",
      ".:.\n"
     ]
    }
   ],
   "source": [
    "# Stemming a sentence\n",
    "example_sentence=\"It is important to learn what you are learning.\"\n",
    "words = word_tokenize(example_sentence)\n",
    "ps = PorterStemmer()\n",
    "for w in words:\n",
    "    print (w+':'+ps.stem(w) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Part', 'NN'), ('of', 'IN'), ('Speech', 'NNP'), ('tagging', 'NN'), ('does', 'VBZ'), ('exactly', 'RB'), ('what', 'WP'), ('it', 'PRP'), ('sounds', 'VBZ'), ('like', 'IN'), (',', ','), ('it', 'PRP'), ('tags', 'VBZ'), ('each', 'DT'), ('word', 'NN'), ('in', 'IN'), ('a', 'DT'), ('sentence', 'NN'), ('with', 'IN'), ('the', 'DT'), ('part', 'NN'), ('of', 'IN'), ('speech', 'NN'), ('for', 'IN'), ('that', 'DT'), ('word', 'NN'), ('.', '.')]\n",
      "[('This', 'DT'), ('means', 'VBZ'), ('it', 'PRP'), ('labels', 'VBZ'), ('words', 'NNS'), ('as', 'IN'), ('noun', 'NN'), (',', ','), ('adjective', 'JJ'), (',', ','), ('verb', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.')]\n",
      "[('PoS', 'NNP'), ('tagging', 'VBG'), ('also', 'RB'), ('covers', 'VBZ'), ('tenses', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#Part of Speech\n",
    "\n",
    "sample=\"Part of Speech tagging does exactly what it sounds like, it tags each word in a sentence with the part of speech for that word. This means it labels words as noun, adjective, verb, etc. PoS tagging also covers tenses of the parts of speech.\"\n",
    "\n",
    "tokenized = sent_tokenize(sample)\n",
    "for i in tokenized:\n",
    "    words = word_tokenize(i)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    print (tagged)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent\\'s\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
